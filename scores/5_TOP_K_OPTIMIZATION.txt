â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     TOP-K VALUE OPTIMIZATION ANALYSIS                        â•‘
â•‘                    RAGAS Metrics - February 23, 2026                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CRITICAL FINDING: Top-K Changes Won't Fix Multi-Agent or Hybrid Issues
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

The core problems are:
â”œâ”€ Multi-Agent:  SYNTHESIS HALLUCINATION (not retrieval)
â”œâ”€ Hybrid:       METADATA FILTERING TOO AGGRESSIVE (not retrieval)
â””â”€ Single:       ALREADY OPTIMAL (don't change anything)


PART 1: UNDERSTANDING THE METRICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The 5 RAGAS Metrics:

1. context_precision: % of retrieved docs relevant to query
   â””â”€ All agents = 0.800 âœ“ (equally good retrieval quality)

2. context_recall: % of ground truth docs retrieved
   â””â”€ Single=0.767, Multi=0.700, Hybrid=0.667 (filtering reduces recall)

3. faithfulness: % of answer grounded in retrieved context â­ MOST IMPORTANT
   â””â”€ Single=0.827 âœ“, Hybrid=0.685 â—‹, Multi=0.558 âœ— (CRISIS!)

4. answer_relevancy: % of answer addressing the question
   â””â”€ Multi=0.827 âœ“, Single=0.798 â—‹, Hybrid=0.626 âœ—

5. answer_correctness: % of answer factually accurate
   â””â”€ Single=0.708 âœ“, Multi=0.706 â—‹, Hybrid=0.646 âœ—


CRITICAL INSIGHT: Why Multi-Agent Seems Good But Isn't
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Multi-Agent Scores:
â”œâ”€ answer_relevancy: 0.827 â­ HIGHEST!
â”œâ”€ answer_correctness: 0.706 (nearly tied with Single)
â””â”€ faithfulness: 0.558 âœ— THE HIDDEN PROBLEM!

What's happening:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Agent generates "relevant" answers    â”‚
â”‚ BUT 44% of the answer is HALLUCINATED      â”‚
â”‚ (not grounded in retrieved documents)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Why? The supervisor SYNTHESIZES answers     â”‚
â”‚ combining multiple sub-agent outputs,       â”‚
â”‚ adding new information not in the docs.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: Sounds good, but can't be trusted for legal applications!


PART 2: TOP-K OPTIMIZATION BY AGENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ SINGLE AGENT RAG: Top-K = 10 âœ“ (OPTIMAL - DON'T CHANGE)
â”‚
â”‚  Current Performance (Top-K=10):
â”‚  â”œâ”€ context_precision: 0.800 âœ“ Good
â”‚  â”œâ”€ context_recall: 0.767 âœ“ Best
â”‚  â”œâ”€ faithfulness: 0.827 âœ“ BEST
â”‚  â”œâ”€ answer_relevancy: 0.798 âœ“ Strong
â”‚  â””â”€ answer_correctness: 0.708 âœ“ BEST
â”‚
â”‚  What if Top-K = 5?
â”‚  â”œâ”€ Fewer documents retrieved
â”‚  â”œâ”€ Recall drops: 0.767 â†’ ~0.71-0.73
â”‚  â”œâ”€ Precision improves: 0.800 â†’ ~0.81-0.82
â”‚  â”œâ”€ Faithfulness improves: 0.827 â†’ ~0.83-0.84 (higher quality)
â”‚  â””â”€ Verdict: NARROWER, not necessarily better
â”‚
â”‚  What if Top-K = 15?
â”‚  â”œâ”€ More documents retrieved
â”‚  â”œâ”€ Recall improves: 0.767 â†’ ~0.78-0.80
â”‚  â”œâ”€ Precision stays: 0.800 (still good)
â”‚  â”œâ”€ Faithfulness drops: 0.827 â†’ ~0.80-0.82 (marginal quality docs)
â”‚  â””â”€ Verdict: BROADER, adds noise
â”‚
â”‚  RECOMMENDATION: KEEP Top-K = 10
â”‚  â””â”€ This is the sweet spot for Single Agent
â”‚     - Precision and recall well-balanced
â”‚     - Faithfulness excellent
â”‚     - No changes needed


â”œâ”€ MULTI-AGENT RAG: Top-K = 10 (NOT THE PROBLEM)
â”‚
â”‚  Current Performance (Top-K=10):
â”‚  â”œâ”€ context_precision: 0.800 âœ“ Good (same as Single!)
â”‚  â”œâ”€ context_recall: 0.700 â—‹ Lost 6.7% vs Single
â”‚  â”œâ”€ faithfulness: 0.558 âœ—âœ— CRISIS (-27% vs Single!)
â”‚  â”œâ”€ answer_relevancy: 0.827 â­ BEST
â”‚  â””â”€ answer_correctness: 0.706 â—‹ Nearly same as Single
â”‚
â”‚  Analysis: THE PROBLEM IS NOT RETRIEVAL
â”‚  â””â”€ Precision is good (0.800) = retrieving right docs
â”‚     Recall lost 6.7% due to routing (expected)
â”‚     BUT faithfulness lost 27% = SYNTHESIS adding hallucinations
â”‚
â”‚  What if Top-K = 5?
â”‚  â”œâ”€ Recall worsens: 0.700 â†’ ~0.65-0.68
â”‚  â”œâ”€ Faithfulness stays: ~0.558 (synthesis still hallucinates)
â”‚  â””â”€ Verdict: MAKES IT WORSE âœ—
â”‚
â”‚  What if Top-K = 7?
â”‚  â”œâ”€ Recall: ~0.68-0.70 (minimal gain)
â”‚  â”œâ”€ Faithfulness stays: ~0.558 (synthesis still hallucinates)
â”‚  â””â”€ Verdict: NO IMPROVEMENT âœ—
â”‚
â”‚  What if Top-K = 15?
â”‚  â”œâ”€ Recall: ~0.71-0.73 (marginal improvement)
â”‚  â”œâ”€ Faithfulness: ~0.54-0.56 (MORE hallucination material)
â”‚  â””â”€ Verdict: COUNTERPRODUCTIVE âœ—
â”‚
â”‚  ROOT CAUSE: Supervisor synthesis is creating hallucinations
â”‚  â”œâ”€ Sub-agents retrieve docs correctly
â”‚  â”œâ”€ Sub-agents generate answers from docs
â”‚  â”œâ”€ Supervisor combines answers + generates new content
â”‚  â””â”€ New content = hallucinated (not in retrieved docs)
â”‚
â”‚  RECOMMENDATION: DON'T CHANGE Top-K
â”‚  â”œâ”€ Top-K is not the problem
â”‚  â”œâ”€ Fix synthesis architecture (1-2 months):
â”‚  â”‚  â”œâ”€ Constrain supervisor to aggregation only
â”‚  â”‚  â”œâ”€ No new content generation
â”‚  â”‚  â”œâ”€ Require evidence for each claim
â”‚  â”‚  â””â”€ Add confidence scoring
â”‚  â”œâ”€ After fixing synthesis, re-evaluate all metrics
â”‚  â””â”€ THEN consider Top-K optimization if needed


â””â”€ HYBRID LEGAL RAG: Top-K = 10 (SECONDARY ISSUE)
   
   Current Performance (Top-K=10):
   â”œâ”€ context_precision: 0.800 âœ“ Good
   â”œâ”€ context_recall: 0.667 âœ— WORST (-10% vs Single)
   â”œâ”€ faithfulness: 0.685 â—‹ Moderate
   â”œâ”€ answer_relevancy: 0.626 âœ— Lowest
   â””â”€ answer_correctness: 0.646 âœ— Lowest
   
   Analysis: THE PROBLEM IS METADATA FILTERING
   â””â”€ Precision is good (0.800) = initial retrieval OK
      Recall lost 10% = metadata filters removing relevant docs
      Faithfulness lost 14% = confidence issues
   
   What if Top-K = 5?
   â”œâ”€ Recall worsens: 0.667 â†’ ~0.60-0.63
   â”œâ”€ Even fewer docs survive filtering
   â””â”€ Verdict: WORSE âœ—
   
   What if Top-K = 15?
   â”œâ”€ More base docs to filter from
   â”œâ”€ Might recover some recall: 0.667 â†’ ~0.70-0.72
   â”œâ”€ BUT adds noise (wrong metadata matches)
   â””â”€ Verdict: MARGINAL, only after fixing filtering
   
   ROOT CAUSE: Metadata filtering too aggressive
   â”œâ”€ Hard filters remove potentially relevant docs
   â”œâ”€ Schema constraints conflict with actual questions
   â””â”€ Result: Lower recall, lower correctness
   
   RECOMMENDATION: FIX FILTERING FIRST
   â”œâ”€ Phase 1 (Weeks 1-4): Soft filtering redesign
   â”‚  â”œâ”€ Replace hard filters with scoring (0-1)
   â”‚  â”œâ”€ Metadata as ranking signal, not blocker
   â”‚  â”œâ”€ Implement fallback to semantic-only
   â”‚  â””â”€ Expected: recall 0.667 â†’ 0.72-0.75
   â”œâ”€ Phase 2 (Weeks 5-8): Schema simplification
   â”‚  â”œâ”€ Fewer required fields
   â”‚  â”œâ”€ Flexible extraction
   â”‚  â””â”€ Error recovery
   â”œâ”€ Phase 3 (Weeks 9-12): THEN optimize Top-K
   â”‚  â”œâ”€ Test Top-K: 10, 12, 15
   â”‚  â””â”€ Choose based on precision-recall balance
   â””â”€ Phases must be sequential (filtering fix first)


PART 3: TOP-K RECOMMENDATION MATRIX
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                  Top-K=5    Top-K=7    Top-K=10   Top-K=12   Top-K=15
                  â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€
Single Agent:     âœ—Worse     âœ—Worse     âœ“KEEP      â—‹Test      â—‹Test
Multi-Agent:      âœ—Worse     âœ—Worse     âœ“KEEP      âœ“FIX 1ST   âœ“FIX 1ST
Hybrid:           âœ—Worse     âœ—Worse     âœ“FIX 1ST   â—‹FIX 1ST   â—‹FIX 1ST

Legend:
â”œâ”€ âœ“KEEP    = Current value is optimal
â”œâ”€ âœ“FIX 1ST = Keep at 10, fix root cause first, then test
â”œâ”€ â—‹Test    = Can test after root cause fixed
â””â”€ âœ—Worse   = Would degrade performance


PART 4: THE BOTTOM LINE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

QUESTION 1: Is Single Agent really the best?
ANSWER:    âœ“ YES, mathematically proven

Score Ranking:
â”œâ”€ 1. Single Agent RAG: 0.78 / 1.00 (78%)
â”‚  â””â”€ Faithfulness: 0.827 âœ“ BEST
â”‚  â””â”€ Correctness: 0.708 âœ“ BEST  
â”‚  â””â”€ Recall: 0.767 âœ“ BEST
â”‚  â””â”€ Status: âœ“ PRODUCTION READY

â”œâ”€ 2. Hybrid Legal RAG: 0.68 / 1.00 (68%)
â”‚  â””â”€ Problems: Over-filtered, lowest correctness (0.646)
â”‚  â””â”€ Status: â¸ï¸ SHELVED

â””â”€ 3. Multi-Agent RAG: 0.72 / 1.00 (72%)
   â””â”€ Problems: Hallucination crisis, lowest faithfulness (0.558)
   â””â”€ Status: âœ— DO NOT DEPLOY

QUESTION 2: Should we increase Top-K from 10?
ANSWER:    âœ— NO

â”œâ”€ Single Agent: Keep at 10 (already optimal)
â”œâ”€ Multi-Agent: Keep at 10 (fix synthesis, not retrieval)
â””â”€ Hybrid: Keep at 10 (fix filtering, not retrieval)

Changing Top-K alone won't fix the real problems.

QUESTION 3: Should we decrease Top-K to 5 or 7?
ANSWER:    âœ— NO

â”œâ”€ Single Agent: Would lose coverage unnecessarily
â”œâ”€ Multi-Agent: Would make recall worse (already lost 6.7%)
â””â”€ Hybrid: Would make recall worse (already lost 10%)

No benefit, only downside.

QUESTION 4: Is Multi-Agent better than Single?
ANSWER:    âœ— NO

Multi-Agent appears competitive on:
â”œâ”€ answer_relevancy: 0.827 vs 0.798 (Multi +0.029)
â””â”€ answer_correctness: 0.706 vs 0.708 (Single +0.002)

But fails critically on:
â”œâ”€ faithfulness: 0.558 vs 0.827 (Single +0.269 = 27% better!)
â”œâ”€ This means: 44% of Multi-Agent answers are hallucinated
â””â”€ For legal applications: UNACCEPTABLE

QUESTION 5: Can Multi-Agent catch up to Single?
ANSWER:    âœ“ YES, but requires 1-2 months of fixes

Current problems:
â”œâ”€ Supervisor synthesis creates hallucinations
â”œâ”€ NOT a retrieval problem (precision = 0.800, same as Single)
â”œâ”€ NOT a Top-K problem (changing it won't help)
â””â”€ IS an architecture problem (synthesis design)

How to fix:
â”œâ”€ Phase 1: Constrain supervisor to aggregation only
â”œâ”€ Phase 2: Add confidence scoring & evidence requirements
â”œâ”€ Phase 3: Implement multi-voting for controversial claims
â””â”€ Expected result: faithfulness â‰¥ 0.75

Timeline:
â”œâ”€ Weeks 1-2: Analysis & design
â”œâ”€ Weeks 3-6: Implementation
â”œâ”€ Weeks 7-8: Testing & re-evaluation
â””â”€ Total: 2 months


PART 5: IMPLEMENTATION STEPS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… IMMEDIATE (This Week)

Step 1: Deploy Single Agent RAG
â”œâ”€ Use Top-K = 10 (don't change)
â”œâ”€ Metrics are best across board
â”œâ”€ Production-ready now

Step 2: Stop Multi-Agent production use
â”œâ”€ Mark as experimental/research only
â”œâ”€ Document hallucination issue (faithfulness = 0.558)
â”œâ”€ Schedule redesign for next month

Step 3: Remove Hybrid from production path
â”œâ”€ Archive current implementation
â”œâ”€ Mark for redesign (soft filtering first)
â”œâ”€ Don't change Top-K before fixing filtering

Step 4: DO NOT change Top-K
â”œâ”€ Changing it won't solve core issues
â”œâ”€ Single Agent already optimal at 10
â”œâ”€ Multi-Agent & Hybrid need architecture fixes, not retrieval tuning


â³ SHORT-TERM (1-2 Months)

Step 1: Fix Multi-Agent synthesis
â”œâ”€ Week 1-2: Audit supervisor code for hallucination source
â”œâ”€ Week 3-4: Redesign for aggregation-only synthesis
â”œâ”€ Week 5-6: Test on same 30 QA pairs
â”œâ”€ Week 7-8: Verify faithfulness â‰¥ 0.75 target

Step 2: Plan Hybrid redesign
â”œâ”€ Document soft-filtering requirements
â”œâ”€ Simplify schema (fewer constraints)
â”œâ”€ Design fallback strategy

Step 3: Re-evaluate after fixes
â”œâ”€ Measure Multi-Agent faithfulness (target: 0.75+)
â”œâ”€ Measure Hybrid metrics (soft filtering impact)
â”œâ”€ Decide: deploy improved version or continue with Single


ğŸ“Š METRICS TO MONITOR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For Single Agent (maintain current performance):
â”œâ”€ Faithfulness: Keep â‰¥ 0.82
â”œâ”€ Correctness: Keep â‰¥ 0.70
â””â”€ Recall: Keep â‰¥ 0.76

For Multi-Agent (improve to production-ready):
â”œâ”€ Faithfulness: Increase 0.558 â†’ 0.75+ (target: 0.80)
â”œâ”€ Answer_correctness: Maintain â‰¥ 0.70
â””â”€ Hallucination rate: Reduce to < 5%

For Hybrid (fix to usable):
â”œâ”€ Recall: Increase 0.667 â†’ 0.75+
â”œâ”€ Faithfulness: Increase 0.685 â†’ 0.75+
â””â”€ Correctness: Increase 0.646 â†’ 0.70+


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FINAL VERDICT: Keep Top-K=10 for all agents. Fix architecture, not retrieval.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
